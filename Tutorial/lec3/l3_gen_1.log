Creating initial loop nests...
Injecting realization of { gradient }
Skipping injecting memoization...
Injecting tracing...
Adding checks for parameters
Computing bounds of each function's value
Adding checks for images
Performing computation bounds inference...
Removing extern loops...
Performing sliding window optimization...
Simplifying correlated differences...
Performing allocation bounds inference...
Removing code that depends on undef values...
Uniquifying variable names...
Simplifying...
Performing storage folding optimization...
Injecting debug_to_file calls...
Injecting prefetches...
Discarding safe promises...
Dynamically skipping stages...
Forking asynchronous producers...
Destructuring tuple-valued realizations...
Performing storage flattening...
Adding atomic mutex allocation...
Unpacking buffer arguments...
Skipping rewriting memoized allocations...
Simplifying...
Reduce prefetch dimension...
Simplifying correlated differences...
Unrolling...
Vectorizing...
Detecting vector interleavings...
Partitioning loops to simplify boundary conditions...
Trimming loops to the region over which they do something...
Injecting early frees...
Simplifying correlated differences...
Bounding small allocations...
Simplifying...
Lowering unsafe promises...
Lowering after final simplification:
assert(((uint64)reinterpret(((halide_buffer_t *))gradient.buffer) != (uint64)0), halide_error_buffer_argument_is_null("gradient"))
let gradient = ((void *))_halide_buffer_get_host(((halide_buffer_t *))gradient.buffer)
let gradient.type = (uint32)_halide_buffer_get_type(((halide_buffer_t *))gradient.buffer)
let gradient.device_dirty = (uint1)_halide_buffer_get_device_dirty(((halide_buffer_t *))gradient.buffer)
let gradient.dimensions = _halide_buffer_get_dimensions(((halide_buffer_t *))gradient.buffer)
let gradient.min.0 = _halide_buffer_get_min(((halide_buffer_t *))gradient.buffer, 0)
let gradient.extent.0 = _halide_buffer_get_extent(((halide_buffer_t *))gradient.buffer, 0)
let gradient.stride.0 = _halide_buffer_get_stride(((halide_buffer_t *))gradient.buffer, 0)
let gradient.min.1 = _halide_buffer_get_min(((halide_buffer_t *))gradient.buffer, 1)
let gradient.extent.1 = _halide_buffer_get_extent(((halide_buffer_t *))gradient.buffer, 1)
let gradient.stride.1 = _halide_buffer_get_stride(((halide_buffer_t *))gradient.buffer, 1)
if ((uint1)_halide_buffer_is_bounds_query(((halide_buffer_t *))gradient.buffer)) {
 ((halide_buffer_t *))_halide_buffer_init(((halide_buffer_t *))gradient.buffer, ((halide_dimension_t *))_halide_buffer_get_shape(((halide_buffer_t *))gradient.buffer), ((void *))reinterpret((uint64)0), (uint64)0, ((halide_device_interface_t *))reinterpret((uint64)0), 0, 32, 2, ((halide_dimension_t *))make_struct(gradient.min.0, gradient.extent.0, 1, 0, gradient.min.1, gradient.extent.1, gradient.extent.0, 0), (uint64)0)
}
if (!(uint1)_halide_buffer_is_bounds_query(((halide_buffer_t *))gradient.buffer)) {
 assert((gradient.type == (uint32)73728), halide_error_bad_type("Output buffer gradient", gradient.type, (uint32)73728))
 assert((gradient.dimensions == 2), halide_error_bad_dimensions("Output buffer gradient", gradient.dimensions, 2))
 assert((0 <= gradient.extent.0), halide_error_buffer_extents_negative("Output buffer gradient", 0, gradient.extent.0))
 assert((0 <= gradient.extent.1), halide_error_buffer_extents_negative("Output buffer gradient", 1, gradient.extent.1))
 assert((gradient.stride.0 == 1), halide_error_constraint_violated("gradient.stride.0", gradient.stride.0, "1", 1))
 let gradient.total_extent.1 = (int64(gradient.extent.1)*int64(gradient.extent.0))
 assert(((uint64)abs(int64(gradient.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("gradient", (uint64)abs(int64(gradient.extent.0)), (uint64)2147483647))
 assert(((uint64)abs((int64(gradient.extent.1)*int64(gradient.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("gradient", (uint64)abs((int64(gradient.extent.1)*int64(gradient.stride.1))), (uint64)2147483647))
 assert((gradient.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("gradient", gradient.total_extent.1, (int64)2147483647))
 assert(!gradient.device_dirty, halide_error_device_dirty_with_no_device_support("Output buffer gradient"))
 assert((gradient != ((void *))reinterpret((uint64)0)), halide_error_host_is_null("Output buffer gradient"))
 produce gradient {
  let t1 = ((gradient.min.1*gradient.stride.1) + gradient.min.0)
  for (gradient.s0.y, gradient.min.1, gradient.extent.1) {
   let t2 = ((gradient.s0.y*gradient.stride.1) - t1)
   for (gradient.s0.x, gradient.min.0, gradient.extent.0) {
    gradient[(gradient.s0.x + t2)] = (gradient.s0.x + gradient.s0.y)
   }
  }
 }
}


Skipping Hexagon offload...
Target triple of initial module: x86_64--linux-gnu
Generating llvm bitcode...
Generating llvm bitcode prolog for function gradient...
Generating llvm bitcode for function gradient...
JIT compiling shared runtime for x86-64-linux-avx-avx2-f16c-fma-jit-sse41-user_context
JIT compiling gradient for x86-64-linux-avx-avx2-f16c-fma-jit-sse41-user_context
Creating initial loop nests...
Injecting realization of { gradient }
Skipping injecting memoization...
Injecting tracing...
Adding checks for parameters
Computing bounds of each function's value
Adding checks for images
Performing computation bounds inference...
Removing extern loops...
Performing sliding window optimization...
Simplifying correlated differences...
Performing allocation bounds inference...
Removing code that depends on undef values...
Uniquifying variable names...
Simplifying...
Performing storage folding optimization...
Injecting debug_to_file calls...
Injecting prefetches...
Discarding safe promises...
Dynamically skipping stages...
Forking asynchronous producers...
Destructuring tuple-valued realizations...
Performing storage flattening...
Adding atomic mutex allocation...
Unpacking buffer arguments...
Skipping rewriting memoized allocations...
Simplifying...
Reduce prefetch dimension...
Simplifying correlated differences...
Unrolling...
Vectorizing...
Detecting vector interleavings...
Partitioning loops to simplify boundary conditions...
Trimming loops to the region over which they do something...
Injecting early frees...
Simplifying correlated differences...
Bounding small allocations...
Simplifying...
Lowering unsafe promises...
Lowering after final simplification:
assert(((uint64)reinterpret(((halide_buffer_t *))gradient.buffer) != (uint64)0), halide_error_buffer_argument_is_null("gradient"))
let gradient = ((void *))_halide_buffer_get_host(((halide_buffer_t *))gradient.buffer)
let gradient.type = (uint32)_halide_buffer_get_type(((halide_buffer_t *))gradient.buffer)
let gradient.device_dirty = (uint1)_halide_buffer_get_device_dirty(((halide_buffer_t *))gradient.buffer)
let gradient.dimensions = _halide_buffer_get_dimensions(((halide_buffer_t *))gradient.buffer)
let gradient.min.0 = _halide_buffer_get_min(((halide_buffer_t *))gradient.buffer, 0)
let gradient.extent.0 = _halide_buffer_get_extent(((halide_buffer_t *))gradient.buffer, 0)
let gradient.stride.0 = _halide_buffer_get_stride(((halide_buffer_t *))gradient.buffer, 0)
let gradient.min.1 = _halide_buffer_get_min(((halide_buffer_t *))gradient.buffer, 1)
let gradient.extent.1 = _halide_buffer_get_extent(((halide_buffer_t *))gradient.buffer, 1)
let gradient.stride.1 = _halide_buffer_get_stride(((halide_buffer_t *))gradient.buffer, 1)
if ((uint1)_halide_buffer_is_bounds_query(((halide_buffer_t *))gradient.buffer)) {
 ((halide_buffer_t *))_halide_buffer_init(((halide_buffer_t *))gradient.buffer, ((halide_dimension_t *))_halide_buffer_get_shape(((halide_buffer_t *))gradient.buffer), ((void *))reinterpret((uint64)0), (uint64)0, ((halide_device_interface_t *))reinterpret((uint64)0), 0, 32, 2, ((halide_dimension_t *))make_struct(gradient.min.0, gradient.extent.0, 1, 0, gradient.min.1, gradient.extent.1, gradient.extent.0, 0), (uint64)0)
}
if (!(uint1)_halide_buffer_is_bounds_query(((halide_buffer_t *))gradient.buffer)) {
 assert((gradient.type == (uint32)73728), halide_error_bad_type("Output buffer gradient", gradient.type, (uint32)73728))
 assert((gradient.dimensions == 2), halide_error_bad_dimensions("Output buffer gradient", gradient.dimensions, 2))
 assert((0 <= gradient.extent.0), halide_error_buffer_extents_negative("Output buffer gradient", 0, gradient.extent.0))
 assert((0 <= gradient.extent.1), halide_error_buffer_extents_negative("Output buffer gradient", 1, gradient.extent.1))
 assert((gradient.stride.0 == 1), halide_error_constraint_violated("gradient.stride.0", gradient.stride.0, "1", 1))
 let gradient.total_extent.1 = (int64(gradient.extent.1)*int64(gradient.extent.0))
 assert(((uint64)abs(int64(gradient.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("gradient", (uint64)abs(int64(gradient.extent.0)), (uint64)2147483647))
 assert(((uint64)abs((int64(gradient.extent.1)*int64(gradient.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("gradient", (uint64)abs((int64(gradient.extent.1)*int64(gradient.stride.1))), (uint64)2147483647))
 assert((gradient.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("gradient", gradient.total_extent.1, (int64)2147483647))
 assert(!gradient.device_dirty, halide_error_device_dirty_with_no_device_support("Output buffer gradient"))
 assert((gradient != ((void *))reinterpret((uint64)0)), halide_error_host_is_null("Output buffer gradient"))
 produce gradient {
  let t4 = ((gradient.min.1*gradient.stride.1) + gradient.min.0)
  for (gradient.s0.y, gradient.min.1, gradient.extent.1) {
   let t5 = ((gradient.s0.y*gradient.stride.1) - t4)
   for (gradient.s0.x, gradient.min.0, gradient.extent.0) {
    gradient[(gradient.s0.x + t5)] = (gradient.s0.x + gradient.s0.y)
   }
  }
 }
}


Skipping Hexagon offload...
Module.compile(): stmt_html gradient.html
Success!
